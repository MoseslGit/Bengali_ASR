{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c42f1dd5",
   "metadata": {
    "papermill": {
     "duration": 0.007725,
     "end_time": "2023-07-18T15:13:12.668585",
     "exception": false,
     "start_time": "2023-07-18T15:13:12.660860",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference code for YellowKing's model from  DL Sprint 2022\n",
    "https://www.kaggle.com/code/sameen53/yellowking-dlsprint-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82ac5e00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-18T15:14:45.954003Z",
     "iopub.status.busy": "2023-07-18T15:14:45.953638Z",
     "iopub.status.idle": "2023-07-18T15:14:57.229230Z",
     "shell.execute_reply": "2023-07-18T15:14:57.228045Z"
    },
    "papermill": {
     "duration": 11.292196,
     "end_time": "2023-07-18T15:14:57.231997",
     "exception": false,
     "start_time": "2023-07-18T15:14:45.939801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ml/anaconda3/envs/ml_audio/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from glob import glob\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import IPython\n",
    "from datasets import load_metric\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import gc\n",
    "import wave\n",
    "from scipy.io import wavfile\n",
    "import scipy.signal as sps\n",
    "import pyctcdecode\n",
    "\n",
    "tqdm.pandas()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a88d9d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-18T15:14:57.261073Z",
     "iopub.status.busy": "2023-07-18T15:14:57.258832Z",
     "iopub.status.idle": "2023-07-18T15:14:57.274833Z",
     "shell.execute_reply": "2023-07-18T15:14:57.273618Z"
    },
    "papermill": {
     "duration": 0.032006,
     "end_time": "2023-07-18T15:14:57.277044",
     "exception": false,
     "start_time": "2023-07-18T15:14:57.245038",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test_mp3s/0f3dac00655e.mp3', 'test_mp3s/a9395e01ad21.mp3']\n"
     ]
    }
   ],
   "source": [
    "# CHANGE ACCORDINGLY\n",
    "BATCH_SIZE = 16\n",
    "TEST_DIRECTORY = 'test_mp3s'\n",
    "paths = glob(os.path.join(TEST_DIRECTORY,'*.mp3'))\n",
    "print(paths[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6012e0df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-18T15:14:57.303955Z",
     "iopub.status.busy": "2023-07-18T15:14:57.303634Z",
     "iopub.status.idle": "2023-07-18T15:14:57.308541Z",
     "shell.execute_reply": "2023-07-18T15:14:57.307417Z"
    },
    "papermill": {
     "duration": 0.021134,
     "end_time": "2023-07-18T15:14:57.310779",
     "exception": false,
     "start_time": "2023-07-18T15:14:57.289645",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    my_model_name = 'YellowKing_model'\n",
    "    processor_name = 'YellowKing_processor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99f62fe6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-18T15:14:57.337386Z",
     "iopub.status.busy": "2023-07-18T15:14:57.336691Z",
     "iopub.status.idle": "2023-07-18T15:16:33.195451Z",
     "shell.execute_reply": "2023-07-18T15:16:33.194318Z"
    },
    "papermill": {
     "duration": 95.874952,
     "end_time": "2023-07-18T15:16:33.198390",
     "exception": false,
     "start_time": "2023-07-18T15:14:57.323438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ProcessorWithLM\n",
    "\n",
    "processor = Wav2Vec2ProcessorWithLM.from_pretrained(CFG.processor_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f0b8cc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-18T15:16:33.225303Z",
     "iopub.status.busy": "2023-07-18T15:16:33.224976Z",
     "iopub.status.idle": "2023-07-18T15:16:53.937314Z",
     "shell.execute_reply": "2023-07-18T15:16:53.936273Z"
    },
    "papermill": {
     "duration": 20.728659,
     "end_time": "2023-07-18T15:16:53.940017",
     "exception": false,
     "start_time": "2023-07-18T15:16:33.211358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m my_asrLM \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39m\u001b[39mautomatic-speech-recognition\u001b[39m\u001b[39m\"\u001b[39m, model\u001b[39m=\u001b[39mCFG\u001b[39m.\u001b[39mmy_model_name ,feature_extractor \u001b[39m=\u001b[39mprocessor\u001b[39m.\u001b[39mfeature_extractor, tokenizer\u001b[39m=\u001b[39m processor\u001b[39m.\u001b[39mtokenizer,decoder\u001b[39m=\u001b[39mprocessor\u001b[39m.\u001b[39mdecoder ,device\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_audio/lib/python3.11/site-packages/transformers/pipelines/__init__.py:988\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    986\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m device\n\u001b[0;32m--> 988\u001b[0m \u001b[39mreturn\u001b[39;00m pipeline_class(model\u001b[39m=\u001b[39mmodel, framework\u001b[39m=\u001b[39mframework, task\u001b[39m=\u001b[39mtask, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_audio/lib/python3.11/site-packages/transformers/pipelines/automatic_speech_recognition.py:203\u001b[0m, in \u001b[0;36mAutomaticSpeechRecognitionPipeline.__init__\u001b[0;34m(self, feature_extractor, decoder, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    197\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    198\u001b[0m     feature_extractor: Union[\u001b[39m\"\u001b[39m\u001b[39mSequenceFeatureExtractor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    202\u001b[0m ):\n\u001b[0;32m--> 203\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_extractor \u001b[39m=\u001b[39m feature_extractor\n\u001b[1;32m    206\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mmodel_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mwhisper\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_audio/lib/python3.11/site-packages/transformers/pipelines/base.py:781\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[0;34m(self, model, tokenizer, feature_extractor, image_processor, modelcard, framework, task, args_parser, device, torch_dtype, binary_output, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m=\u001b[39m framework\n\u001b[1;32m    780\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m device \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(device, \u001b[39mint\u001b[39m) \u001b[39mand\u001b[39;00m device \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[0;32m--> 781\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    783\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    784\u001b[0m     \u001b[39m# `accelerate` device map\u001b[39;00m\n\u001b[1;32m    785\u001b[0m     hf_device_map \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39m\"\u001b[39m\u001b[39mhf_device_map\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_audio/lib/python3.11/site-packages/transformers/modeling_utils.py:1878\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1873\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1874\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`.to` is not supported for `8-bit` models. Please use the model as it is, since the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1875\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1876\u001b[0m     )\n\u001b[1;32m   1877\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1878\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mto(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_audio/lib/python3.11/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply(convert)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_audio/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_audio/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 797 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_audio/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_audio/lib/python3.11/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_audio/lib/python3.11/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_audio/lib/python3.11/site-packages/torch/cuda/__init__.py:239\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    238\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 239\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    240\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "my_asrLM = pipeline(\"automatic-speech-recognition\", model=CFG.my_model_name ,feature_extractor =processor.feature_extractor, tokenizer= processor.tokenizer,decoder=processor.decoder ,device=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cadec1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-18T15:16:53.967812Z",
     "iopub.status.busy": "2023-07-18T15:16:53.967464Z",
     "iopub.status.idle": "2023-07-18T15:16:56.077858Z",
     "shell.execute_reply": "2023-07-18T15:16:56.076475Z"
    },
    "papermill": {
     "duration": 2.127223,
     "end_time": "2023-07-18T15:16:56.080791",
     "exception": false,
     "start_time": "2023-07-18T15:16:53.953568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "speech, sr = librosa.load('/kaggle/input/bengaliai-speech/test_mp3s/0f3dac00655e.mp3', sr=processor.feature_extractor.sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15167c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-18T15:16:56.109607Z",
     "iopub.status.busy": "2023-07-18T15:16:56.108002Z",
     "iopub.status.idle": "2023-07-18T15:17:03.389629Z",
     "shell.execute_reply": "2023-07-18T15:17:03.388617Z"
    },
    "papermill": {
     "duration": 7.297937,
     "end_time": "2023-07-18T15:17:03.391934",
     "exception": false,
     "start_time": "2023-07-18T15:16:56.093997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'একটু বয়স হলে একটি বিদেশি।'}, {'text': 'একটু বয়স হলে একটি বিদেশি।'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_asrLM([speech]*2, chunk_length_s=112, stride_length_s=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30524b11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-18T15:17:03.420400Z",
     "iopub.status.busy": "2023-07-18T15:17:03.418624Z",
     "iopub.status.idle": "2023-07-18T15:17:03.426116Z",
     "shell.execute_reply": "2023-07-18T15:17:03.424912Z"
    },
    "papermill": {
     "duration": 0.023664,
     "end_time": "2023-07-18T15:17:03.428434",
     "exception": false,
     "start_time": "2023-07-18T15:17:03.404770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.automatic_speech_recognition.AutomaticSpeechRecognitionPipeline at 0x7827e12c9f90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_asrLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d18ceb",
   "metadata": {
    "papermill": {
     "duration": 0.01248,
     "end_time": "2023-07-18T15:17:03.455701",
     "exception": false,
     "start_time": "2023-07-18T15:17:03.443221",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Following Sample Submission:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74544daa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-18T15:17:03.482507Z",
     "iopub.status.busy": "2023-07-18T15:17:03.482215Z",
     "iopub.status.idle": "2023-07-18T15:17:03.489296Z",
     "shell.execute_reply": "2023-07-18T15:17:03.488392Z"
    },
    "papermill": {
     "duration": 0.022813,
     "end_time": "2023-07-18T15:17:03.491439",
     "exception": false,
     "start_time": "2023-07-18T15:17:03.468626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, paths):\n",
    "        self.paths = paths\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    def __getitem__(self,idx):\n",
    "        speech, sr = librosa.load(self.paths[idx], sr=processor.feature_extractor.sampling_rate) \n",
    "#         print(speech.shape)\n",
    "        return speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a896b4b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-18T15:17:03.517888Z",
     "iopub.status.busy": "2023-07-18T15:17:03.517597Z",
     "iopub.status.idle": "2023-07-18T15:17:04.507769Z",
     "shell.execute_reply": "2023-07-18T15:17:04.506477Z"
    },
    "papermill": {
     "duration": 1.006114,
     "end_time": "2023-07-18T15:17:04.510240",
     "exception": false,
     "start_time": "2023-07-18T15:17:03.504126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "       -2.2192553e-06,  8.4718761e-07, -2.2282691e-07], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = AudioDataset(paths)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503291fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-18T15:17:04.538708Z",
     "iopub.status.busy": "2023-07-18T15:17:04.537664Z",
     "iopub.status.idle": "2023-07-18T15:17:04.542897Z",
     "shell.execute_reply": "2023-07-18T15:17:04.541866Z"
    },
    "papermill": {
     "duration": 0.02151,
     "end_time": "2023-07-18T15:17:04.545002",
     "exception": false,
     "start_time": "2023-07-18T15:17:04.523492",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79654999",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-18T15:17:04.571787Z",
     "iopub.status.busy": "2023-07-18T15:17:04.571465Z",
     "iopub.status.idle": "2023-07-18T15:17:04.578677Z",
     "shell.execute_reply": "2023-07-18T15:17:04.577749Z"
    },
    "papermill": {
     "duration": 0.023093,
     "end_time": "2023-07-18T15:17:04.580922",
     "exception": false,
     "start_time": "2023-07-18T15:17:04.557829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn_padd(batch):\n",
    "    '''\n",
    "    Padds batch of variable length\n",
    "\n",
    "    note: it converts things ToTensor manually here since the ToTensor transform\n",
    "    assume it takes in images rather than arbitrary tensors.\n",
    "    '''\n",
    "    ## get sequence lengths\n",
    "    lengths = torch.tensor([ t.shape[0] for t in batch ])\n",
    "    ## padd\n",
    "    batch = [ torch.Tensor(t) for t in batch ]\n",
    "    batch = torch.nn.utils.rnn.pad_sequence(batch)\n",
    "    ## compute mask\n",
    "    mask = (batch != 0)\n",
    "    return batch, lengths, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2a3bf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-18T15:17:04.607866Z",
     "iopub.status.busy": "2023-07-18T15:17:04.607573Z",
     "iopub.status.idle": "2023-07-18T15:17:04.613776Z",
     "shell.execute_reply": "2023-07-18T15:17:04.612853Z"
    },
    "papermill": {
     "duration": 0.022135,
     "end_time": "2023-07-18T15:17:04.615883",
     "exception": false,
     "start_time": "2023-07-18T15:17:04.593748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=8, collate_fn=collate_fn_padd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3894b7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-18T15:17:04.642961Z",
     "iopub.status.busy": "2023-07-18T15:17:04.642150Z",
     "iopub.status.idle": "2023-07-18T15:17:08.858320Z",
     "shell.execute_reply": "2023-07-18T15:17:08.856922Z"
    },
    "papermill": {
     "duration": 4.232772,
     "end_time": "2023-07-18T15:17:08.861383",
     "exception": false,
     "start_time": "2023-07-18T15:17:04.628611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds_all = []\n",
    "for batch, lengths, mask in dataloader:\n",
    "    preds = my_asrLM(list(batch.numpy().transpose()))\n",
    "    preds_all+=preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3e3509",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-18T15:17:08.891548Z",
     "iopub.status.busy": "2023-07-18T15:17:08.890591Z",
     "iopub.status.idle": "2023-07-18T15:17:08.912655Z",
     "shell.execute_reply": "2023-07-18T15:17:08.911605Z"
    },
    "papermill": {
     "duration": 0.039098,
     "end_time": "2023-07-18T15:17:08.914989",
     "exception": false,
     "start_time": "2023-07-18T15:17:08.875891",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bnunicodenormalizer import Normalizer \n",
    "\n",
    "\n",
    "bnorm = Normalizer()\n",
    "def normalize(sen):\n",
    "    _words = [bnorm(word)['normalized']  for word in sen.split()]\n",
    "    return \" \".join([word for word in _words if word is not None])\n",
    "\n",
    "def dari(sentence):\n",
    "    try:\n",
    "        if sentence[-1]!=\"।\":\n",
    "            sentence+=\"।\"\n",
    "    except:\n",
    "        print(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec428ea0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-18T15:17:08.942686Z",
     "iopub.status.busy": "2023-07-18T15:17:08.942382Z",
     "iopub.status.idle": "2023-07-18T15:17:08.988657Z",
     "shell.execute_reply": "2023-07-18T15:17:08.987755Z"
    },
    "papermill": {
     "duration": 0.062701,
     "end_time": "2023-07-18T15:17:08.990796",
     "exception": false,
     "start_time": "2023-07-18T15:17:08.928095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df= pd.DataFrame(\n",
    "    {\n",
    "        \"id\":[p.split(os.sep)[-1].replace('.mp3','') for p in paths],\n",
    "        \"sentence\":[p['text']for p in preds_all]\n",
    "    }\n",
    ")\n",
    "df.sentence= df.sentence.apply(lambda x:normalize(x))\n",
    "df.sentence= df.sentence.apply(lambda x:dari(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119d4367",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-18T15:17:09.018384Z",
     "iopub.status.busy": "2023-07-18T15:17:09.017566Z",
     "iopub.status.idle": "2023-07-18T15:17:09.032356Z",
     "shell.execute_reply": "2023-07-18T15:17:09.031358Z"
    },
    "papermill": {
     "duration": 0.030513,
     "end_time": "2023-07-18T15:17:09.034483",
     "exception": false,
     "start_time": "2023-07-18T15:17:09.003970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a9395e01ad21</td>\n",
       "      <td>কী কারণে তুমি এতাবৎ কাল পর্যন্ত এ দারুন দৈব দু...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0f3dac00655e</td>\n",
       "      <td>একটু বয়স হলে একটি বিদেশি।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bf36ea8b718d</td>\n",
       "      <td>এ কারণে সরকার নির্ধারিত হারে পরিবহন জনিত ক্ষতি...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                           sentence\n",
       "0  a9395e01ad21  কী কারণে তুমি এতাবৎ কাল পর্যন্ত এ দারুন দৈব দু...\n",
       "1  0f3dac00655e                          একটু বয়স হলে একটি বিদেশি।\n",
       "2  bf36ea8b718d  এ কারণে সরকার নির্ধারিত হারে পরিবহন জনিত ক্ষতি..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119e7582",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-18T15:17:09.063028Z",
     "iopub.status.busy": "2023-07-18T15:17:09.062217Z",
     "iopub.status.idle": "2023-07-18T15:17:09.070310Z",
     "shell.execute_reply": "2023-07-18T15:17:09.069438Z"
    },
    "papermill": {
     "duration": 0.024329,
     "end_time": "2023-07-18T15:17:09.072457",
     "exception": false,
     "start_time": "2023-07-18T15:17:09.048128",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c04f71a",
   "metadata": {
    "papermill": {
     "duration": 0.012755,
     "end_time": "2023-07-18T15:17:09.098461",
     "exception": false,
     "start_time": "2023-07-18T15:17:09.085706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 249.049292,
   "end_time": "2023-07-18T15:17:12.232886",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-07-18T15:13:03.183594",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
